input {

    file {
        id => "gatling_file_input"
        path => ["/var/tmp/logstash/input/*.log"]
        start_position => "beginning"
    }

}

filter {

    grok {
        id => "gatling_request_grok"
        pattern_definitions => {
            "FIELD" => "[^\t]*"
        }
        match => {
            # Parsing log Gatling de type REQUEST. Les champs sont sÃ©parÃ©s par des tabulations
            # Fields after error_message are custom fields added by Gatling customization
            "message" => "REQUEST\t%{FIELD:scenario_name}\t%{FIELD:gatling_user_id:int}\t%{FIELD:group_name}\t%{FIELD:request_name}\t%{FIELD:start_timestamp:int}\t%{FIELD:end_timestamp:int}\t%{FIELD:status}\t%{FIELD:error_message}\t%{FIELD:correlation_id}\t%{FIELD:request_id}\t%{FIELD:http_status:int}\t%{FIELD:error_code}"
        }
        add_field => {
            "type" => "gatling_request"
        }
        remove_field => ["message"]
    }

    # Compute request duration from timestamps
    if [start_timestamp] and [end_timestamp] {
        ruby {
            id => "gatling_request_duration"
            code => "event.set('duration_ms', event.get('end_timestamp') - event.get('start_timestamp'))"
        }
    }

    # Start timestamp to date
    date {
        id => "gatling_start_date"
        match => ["start_timestamp", "UNIX_MS"]
        target => "@timestamp"
        remove_field => ["start_timestamp"]
    }

    # End timestamp to date
    date {
        id => "gatling_end_date"
        match => ["end_timestamp", "UNIX_MS"]
        target => "end_date"
        remove_field => ["end_timestamp"]
    }

}

output {

    stdout {
        codec => rubydebug
    }

    if [type] == "gatling_request" {
        elasticsearch {
            hosts => ["http://elasticsearch:9200"]
            user => "logstash_writer"
            password => "password"
            http_compression => true
        }
    }

}
